{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deepspeech test1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5in5-owvSyC",
        "colab_type": "text"
      },
      "source": [
        "This is a simple documentation for downloading the DeepSpeech Git repository and using it to train on a new data set using Google colab (CPU only)\n",
        "\n",
        "1. DeepSpeech Documentation [Here](https://deepspeech.readthedocs.io/en/v0.7.4/?badge=latest)\n",
        "\n",
        "2. DeepSpeech Discourse [Here](https://discourse.mozilla.org/t/no-matching-distribution-found-for-tensorflow-1-15-2-from-deepspeech-training-0-7-3/61485/18)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zygf_OJOUHau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHRohbYvbs4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0x_2O51bxT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "94fd9db9-6122-4003-c068-ff27f0d0351c"
      },
      "source": [
        "#mounting google drive\n",
        "drive.mount ('/content/abc')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/abc; to attempt to forcibly remount, call drive.mount(\"/content/abc\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjgk6XBnEfr_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!mkdir "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPwYKt0OEQih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "a5b87a0e-298c-4b30-f0d5-d09cf9a5968c"
      },
      "source": [
        "#https://support.google.com/drive/thread/8914333?hl=en\n",
        "#https://stackoverflow.com/questions/49588113/google-colab-script-throws-transport-endpoint-is-not-connected\n",
        "#!mount --bind /content/drive/My\\ Drive /content/MyDrive\n",
        "\n",
        "#%cd /content/MyDrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVWQra0acomK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#changing current directory\n",
        "os.chdir('/content/sample_data')"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_7s-LcetT1V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "78304eae-875e-409b-f585-d66bd97c9d41"
      },
      "source": [
        "#installing deepspeech Python binary\n",
        "!pip3 install deepspeech"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting deepspeech\n",
            "  Downloading deepspeech-0.7.4-cp36-cp36m-manylinux1_x86_64.whl (9.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech) (1.19.0)\n",
            "Installing collected packages: deepspeech\n",
            "Successfully installed deepspeech-0.7.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV13vaJZUx0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#installing deepspeech pre-trained model and scoring file (check documentation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjG54JKxUx58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pre-trained English model files\n",
        "! curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.pbmm\n",
        "! curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.scorer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdzB2rF0uwBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "1d74714f-593d-4ce3-a6b9-2a09b51b9988"
      },
      "source": [
        "#Installing already provided audio files by Deepspeech for checking model peformance\n",
        "!curl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/audio-0.7.4.tar.gz\n",
        "!tar xvf audio-0.7.4.tar.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   629  100   629    0     0   3656      0 --:--:-- --:--:-- --:--:--  3656\n",
            "\r100  193k  100  193k    0     0   535k      0 --:--:-- --:--:-- --:--:--  535k\n",
            "._audio\n",
            "audio/\n",
            "audio/._2830-3980-0043.wav\n",
            "audio/2830-3980-0043.wav\n",
            "audio/._Attribution.txt\n",
            "audio/Attribution.txt\n",
            "audio/._4507-16021-0012.wav\n",
            "audio/4507-16021-0012.wav\n",
            "audio/._8455-210777-0068.wav\n",
            "audio/8455-210777-0068.wav\n",
            "audio/._License.txt\n",
            "audio/License.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx8BTX38u3kc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "8e6ab87e-e2d9-4e0d-9ac0-c1a40eb295ff"
      },
      "source": [
        "# Transcribe an audio file\n",
        "!deepspeech --model deepspeech-0.7.4-models.pbmm --scorer deepspeech-0.7.4-models.scorer --audio audio/tmp.wav"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file deepspeech-0.7.4-models.pbmm\n",
            "TensorFlow: v1.15.0-24-gceb46aa\n",
            "DeepSpeech: v0.7.4-0-gfcd9563\n",
            "2020-07-03 14:16:03.774166: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.0133s.\n",
            "Loading scorer from files deepspeech-0.7.4-models.scorer\n",
            "Loaded scorer in 0.00567s.\n",
            "Running inference.\n",
            "she had her duck suit and greasy washball year\n",
            "Inference took 3.676s for 2.925s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_TH7oCDc8rq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating model from scratch: for this we are using only 10 audio files from the TIMIT data set, our attempt here is to only check the set-up if it is functioning correctly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olC5PKZE7Y--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "a34081cf-634f-4bcc-91eb-c2c732a408b7"
      },
      "source": [
        "#cloning deepspeech git repository\n",
        "! git clone --branch v0.7.3 https://github.com/mozilla/DeepSpeech"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/40)\u001b[K\rremote: Counting objects:   5% (2/40)\u001b[K\rremote: Counting objects:   7% (3/40)\u001b[K\rremote: Counting objects:  10% (4/40)\u001b[K\rremote: Counting objects:  12% (5/40)\u001b[K\rremote: Counting objects:  15% (6/40)\u001b[K\rremote: Counting objects:  17% (7/40)\u001b[K\rremote: Counting objects:  20% (8/40)\u001b[K\rremote: Counting objects:  22% (9/40)\u001b[K\rremote: Counting objects:  25% (10/40)\u001b[K\rremote: Counting objects:  27% (11/40)\u001b[K\rremote: Counting objects:  30% (12/40)\u001b[K\rremote: Counting objects:  32% (13/40)\u001b[K\rremote: Counting objects:  35% (14/40)\u001b[K\rremote: Counting objects:  37% (15/40)\u001b[K\rremote: Counting objects:  40% (16/40)\u001b[K\rremote: Counting objects:  42% (17/40)\u001b[K\rremote: Counting objects:  45% (18/40)\u001b[K\rremote: Counting objects:  47% (19/40)\u001b[K\rremote: Counting objects:  50% (20/40)\u001b[K\rremote: Counting objects:  52% (21/40)\u001b[K\rremote: Counting objects:  55% (22/40)\u001b[K\rremote: Counting objects:  57% (23/40)\u001b[K\rremote: Counting objects:  60% (24/40)\u001b[K\rremote: Counting objects:  62% (25/40)\u001b[K\rremote: Counting objects:  65% (26/40)\u001b[K\rremote: Counting objects:  67% (27/40)\u001b[K\rremote: Counting objects:  70% (28/40)\u001b[K\rremote: Counting objects:  72% (29/40)\u001b[K\rremote: Counting objects:  75% (30/40)\u001b[K\rremote: Counting objects:  77% (31/40)\u001b[K\rremote: Counting objects:  80% (32/40)\u001b[K\rremote: Counting objects:  82% (33/40)\u001b[K\rremote: Counting objects:  85% (34/40)\u001b[K\rremote: Counting objects:  87% (35/40)\u001b[K\rremote: Counting objects:  90% (36/40)\u001b[K\rremote: Counting objects:  92% (37/40)\u001b[K\rremote: Counting objects:  95% (38/40)\u001b[K\rremote: Counting objects:  97% (39/40)\u001b[K\rremote: Counting objects: 100% (40/40)\u001b[K\rremote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 19410 (delta 8), reused 19 (delta 6), pack-reused 19370\u001b[K\n",
            "Receiving objects: 100% (19410/19410), 47.95 MiB | 25.07 MiB/s, done.\n",
            "Resolving deltas: 100% (13232/13232), done.\n",
            "Note: checking out '88584941bc2ff5b91d6b11ad0a6b85da391d626b'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Rga896Lz7w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "cf661302-62a6-4d9b-f65a-f0aa6280e5bf"
      },
      "source": [
        "#changing curent directory to 'DeepSpeech'\n",
        "cd DeepSpeech"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/sample_data/DeepSpeech\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jln55y3kL3qM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3a574c5f-ff87-4c61-a5d0-7f59039f880b"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/sample_data/DeepSpeech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH_GgFn5fN1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "856f9921-5708-498b-9500-4eb006aa6cfc"
      },
      "source": [
        "#installing all the dependencies\n",
        "#!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "#!pip3 install --upgrade --force-reinstall -e .\n",
        "#!python3 util/taskcluster.py --target ."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.3.cpu/artifacts/public/native_client.tar.xz ...\n",
            "Downloading: 100%\n",
            "\n",
            "libdeepspeech.so\n",
            "LICENSE\n",
            "deepspeech\n",
            "deepspeech.h\n",
            "README.mozilla\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVDIujGbiwNd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56e1736a-118c-439c-a9a0-4616a0c6459c"
      },
      "source": [
        "#this method provides a complete list of all the command line options\n",
        "! python3 DeepSpeech.py --helpfull"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       USAGE: DeepSpeech.py [flags]\n",
            "flags:\n",
            "\n",
            "absl.app:\n",
            "  -?,--[no]help: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpfull: show full help\n",
            "    (default: 'false')\n",
            "  --[no]helpshort: show this help\n",
            "    (default: 'false')\n",
            "  --[no]helpxml: like --helpfull, but generates XML output\n",
            "    (default: 'false')\n",
            "  --[no]only_check_args: Set to true to validate args and exit.\n",
            "    (default: 'false')\n",
            "  --[no]pdb_post_mortem: Set to true to handle uncaught exceptions with PDB post\n",
            "    mortem.\n",
            "    (default: 'false')\n",
            "  --profile_file: Dump profile information to a file (for python -m pstats).\n",
            "    Implies --run_with_profiling.\n",
            "  --[no]run_with_pdb: Set to true for PDB debug mode\n",
            "    (default: 'false')\n",
            "  --[no]run_with_profiling: Set to true for profiling the script. Execution will\n",
            "    be slower, and the output format might change over time.\n",
            "    (default: 'false')\n",
            "  --[no]use_cprofile_for_profiling: Use cProfile instead of the profile module\n",
            "    for profiling. This has no effect unless --run_with_profiling is set.\n",
            "    (default: 'true')\n",
            "\n",
            "absl.logging:\n",
            "  --[no]alsologtostderr: also log to stderr?\n",
            "    (default: 'false')\n",
            "  --log_dir: directory to write logfiles into\n",
            "    (default: '')\n",
            "  --[no]logtostderr: Should only log to stderr?\n",
            "    (default: 'false')\n",
            "  --[no]showprefixforinfo: If False, do not prepend prefix to info messages when\n",
            "    it's logged to stderr, --verbosity is set to INFO level, and python logging\n",
            "    is used.\n",
            "    (default: 'true')\n",
            "  --stderrthreshold: log messages at this level, or more severe, to stderr in\n",
            "    addition to the logfile.  Possible values are 'debug', 'info', 'warning',\n",
            "    'error', and 'fatal'.  Obsoletes --alsologtostderr. Using --alsologtostderr\n",
            "    cancels the effect of this flag. Please also note that this flag is subject\n",
            "    to --verbosity and requires logfile not be stderr.\n",
            "    (default: 'fatal')\n",
            "  -v,--verbosity: Logging verbosity level. Messages logged at this level or\n",
            "    lower will be included. Set to 1 for debug logging. If the flag was not set\n",
            "    or supplied, the value will be changed from the default of -1 (warning) to 0\n",
            "    (info) after flags are parsed.\n",
            "    (default: '-1')\n",
            "    (an integer)\n",
            "\n",
            "absl.testing.absltest:\n",
            "  --test_random_seed: Random seed for testing. Some test frameworks may change\n",
            "    the default value of this flag between runs, so it is not appropriate for\n",
            "    seeding probabilistic tests.\n",
            "    (default: '301')\n",
            "    (an integer)\n",
            "  --test_randomize_ordering_seed: If positive, use this as a seed to randomize\n",
            "    the execution order for test cases. If \"random\", pick a random seed to use.\n",
            "    If 0 or not set, do not randomize test case execution order. This flag also\n",
            "    overrides the TEST_RANDOMIZE_ORDERING_SEED environment variable.\n",
            "    (default: '')\n",
            "  --test_srcdir: Root of directory tree where source files live\n",
            "    (default: '')\n",
            "  --test_tmpdir: Directory for temporary testing files\n",
            "    (default: '/tmp/absl_testing')\n",
            "  --xml_output_file: File to store XML test results\n",
            "    (default: '')\n",
            "\n",
            "deepspeech_training.util.flags:\n",
            "  --alphabet_config_path: path to the configuration file specifying the alphabet\n",
            "    used by the network. See the comment in data/alphabet.txt for a description\n",
            "    of the format.\n",
            "    (default: 'data/alphabet.txt')\n",
            "  --audio_sample_rate: sample rate value expected by model\n",
            "    (default: '16000')\n",
            "    (an integer)\n",
            "  --augment: specifies an augmentation of the training samples. Format is \"--\n",
            "    augment operation[param1=value1, ...]\";\n",
            "    repeat this option to specify a list of values\n",
            "  --[no]augmentation_freq_and_time_masking: whether to use frequency and time\n",
            "    masking augmentation\n",
            "    (default: 'false')\n",
            "  --augmentation_freq_and_time_masking_freq_mask_range: max range of masks in\n",
            "    the frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_freq_masks: number of masks in the\n",
            "    frequency domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_number_time_masks: number of masks in the\n",
            "    time domain when performing freqtime-mask augmentation\n",
            "    (default: '3')\n",
            "    (an integer)\n",
            "  --augmentation_freq_and_time_masking_time_mask_range: max range of masks in\n",
            "    the time domain when performing freqtime-mask augmentation\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --[no]augmentation_pitch_and_tempo_scaling: whether to use spectrogram speed\n",
            "    and tempo scaling\n",
            "    (default: 'false')\n",
            "  --augmentation_pitch_and_tempo_scaling_max_pitch: max value of pitch scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_max_tempo: max vlaue of tempo scaling\n",
            "    (default: '1.2')\n",
            "    (a number)\n",
            "  --augmentation_pitch_and_tempo_scaling_min_pitch: min value of pitch scaling\n",
            "    (default: '0.95')\n",
            "    (a number)\n",
            "  --[no]augmentation_sparse_warp: whether to use spectrogram sparse warp. USE OF\n",
            "    THIS FLAG IS UNSUPPORTED, enable sparse warp will increase training time\n",
            "    drastically, and the paper also mentioned that this is not a major factor to\n",
            "    improve accuracy.\n",
            "    (default: 'false')\n",
            "  --augmentation_sparse_warp_interpolation_order:\n",
            "    sparse_warp_interpolation_order\n",
            "    (default: '2')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_boundary_points:\n",
            "    sparse_warp_num_boundary_points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_num_control_points: specify number of control\n",
            "    points\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --augmentation_sparse_warp_regularization_weight:\n",
            "    sparse_warp_regularization_weight\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --augmentation_sparse_warp_time_warping_para: time_warping_para\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --augmentation_spec_dropout_keeprate: keep rate of dropout augmentation on\n",
            "    spectrogram (if 1, no dropout will be performed on spectrogram)\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --augmentation_speed_up_std: std for speeding-up tempo. If std is 0, this\n",
            "    augmentation is not performed\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --augmentations_per_epoch: how often the train set should be repeated and re-\n",
            "    augmented per epoch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]automatic_mixed_precision: whether to allow automatic mixed precision\n",
            "    training. USE OF THIS FLAG IS UNSUPPORTED. Checkpoints created with\n",
            "    automatic mixed precision training will not be usable without mixed\n",
            "    precision.\n",
            "    (default: 'false')\n",
            "  --beam_width: beam width used in the CTC decoder when building candidate\n",
            "    transcriptions\n",
            "    (default: '1024')\n",
            "    (an integer)\n",
            "  --beta1: beta 1 parameter of Adam optimizer\n",
            "    (default: '0.9')\n",
            "    (a number)\n",
            "  --beta2: beta 2 parameter of Adam optimizer\n",
            "    (default: '0.999')\n",
            "    (a number)\n",
            "  --checkpoint_dir: directory from which checkpoints are loaded and to which\n",
            "    they are saved - defaults to directory \"deepspeech/checkpoints\" within\n",
            "    user's data home specified by the XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --checkpoint_secs: checkpoint saving interval in seconds\n",
            "    (default: '600')\n",
            "    (an integer)\n",
            "  --cutoff_prob: only consider characters until this probability mass is\n",
            "    reached. 1.0 = disabled.\n",
            "    (default: '1.0')\n",
            "    (a number)\n",
            "  --cutoff_top_n: only process this number of characters sorted by probability\n",
            "    mass for each time step. If bigger than alphabet size, disabled.\n",
            "    (default: '300')\n",
            "    (an integer)\n",
            "  --data_aug_features_additive: std of the Gaussian additive noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --data_aug_features_multiplicative: std of normal distribution around 1 for\n",
            "    multiplicative noise\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dev_batch_size: number of elements in a validation batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --dev_files: comma separated list of files specifying the dataset used for\n",
            "    validation. Multiple files will get merged. If empty, validation will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --drop_source_layers: single integer for how many layers to drop from source\n",
            "    model (to drop just output == 1, drop penultimate and output ==2, etc)\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --dropout_rate: dropout rate for feedforward layers\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --dropout_rate2: dropout rate for layer 2 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate3: dropout rate for layer 3 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --dropout_rate4: dropout rate for layer 4 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate5: dropout rate for layer 5 - defaults to 0.0\n",
            "    (default: '0.0')\n",
            "    (a number)\n",
            "  --dropout_rate6: dropout rate for layer 6 - defaults to dropout_rate\n",
            "    (default: '-1.0')\n",
            "    (a number)\n",
            "  --[no]early_stop: Enable early stopping mechanism over validation dataset. If\n",
            "    validation is not being run, early stopping is disabled.\n",
            "    (default: 'false')\n",
            "  --epochs: how many epochs (complete runs through the train files) to train for\n",
            "    (default: '75')\n",
            "    (an integer)\n",
            "  --epsilon: epsilon parameter of Adam optimizer\n",
            "    (default: '1e-08')\n",
            "    (a number)\n",
            "  --es_epochs: Number of epochs with no improvement after which training will be\n",
            "    stopped. Loss is not stored in the checkpoint so when checkpoint is revived\n",
            "    it starts the loss calculation from start at that point\n",
            "    (default: '25')\n",
            "    (an integer)\n",
            "  --es_min_delta: Minimum change in loss to qualify as an improvement. This\n",
            "    value will also be used in Reduce learning rate on plateau\n",
            "    (default: '0.05')\n",
            "    (a number)\n",
            "  --export_author_id: author of the exported model. GitHub user or organization\n",
            "    name used to uniquely identify the author of this model\n",
            "    (default: 'author')\n",
            "  --export_batch_size: number of elements per batch on the exported graph\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --export_beam_width: default beam width to embed into exported graph\n",
            "    (default: '500')\n",
            "    (an integer)\n",
            "  --export_contact_info: public contact information of the author. Can be an\n",
            "    email address, or a link to a contact form, issue tracker, or discussion\n",
            "    forum. Must provide a way to reach the model authors\n",
            "    (default: '<public contact information of the author. Can be an email\n",
            "    address, or a link to a contact form, issue tracker, or discussion forum.\n",
            "    Must provide a way to reach the model authors>')\n",
            "  --export_description: Freeform description of the model being exported.\n",
            "    Markdown accepted. You can also leave this flag unchanged and edit the\n",
            "    generated .md file directly. Useful things to describe are demographic and\n",
            "    acoustic characteristics of the data used to train the model, any\n",
            "    architectural changes, names of public datasets that were used when\n",
            "    applicable, hyperparameters used for training, evaluation results on\n",
            "    standard benchmark datasets, etc.\n",
            "    (default: '<Freeform description of the model being exported. Markdown\n",
            "    accepted. You can also leave this flag unchanged and edit the generated .md\n",
            "    file directly. Useful things to describe are demographic and acoustic\n",
            "    characteristics of the data used to train the model, any architectural\n",
            "    changes, names of public datasets that were used when applicable,\n",
            "    hyperparameters used for training, evaluation results on standard benchmark\n",
            "    datasets, etc.>')\n",
            "  --export_dir: directory in which exported models are stored - if omitted, the\n",
            "    model won't get exported\n",
            "    (default: '')\n",
            "  --export_file_name: name for the exported model file name\n",
            "    (default: 'output_graph')\n",
            "  --export_language: language the model was trained on - IETF BCP 47 language\n",
            "    tag including at least language, script and region subtags. E.g. \"en-Latn-\n",
            "    UK\" or \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can\n",
            "    without loss of precision. For example, if a model is trained on Scottish\n",
            "    English, include the variant subtag: \"en-Latn-GB-Scotland\".\n",
            "    (default: '<language the model was trained on - IETF BCP 47 language tag\n",
            "    including at least language, script and region subtags. E.g. \"en-Latn-UK\" or\n",
            "    \"de-Latn-DE\" or \"cmn-Hans-CN\". Include as much info as you can without loss\n",
            "    of precision. For example, if a model is trained on Scottish English,\n",
            "    include the variant subtag: \"en-Latn-GB-Scotland\".>')\n",
            "  --export_license: SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.\n",
            "    (default: '<SPDX identifier of the license of the exported model. See\n",
            "    https://spdx.org/licenses/. If the license does not have an SPDX identifier,\n",
            "    use the license name.>')\n",
            "  --export_max_ds_version: maximum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<maximum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_min_ds_version: minimum DeepSpeech version (inclusive) the exported\n",
            "    model is compatible with\n",
            "    (default: '<minimum DeepSpeech version (inclusive) the exported model is\n",
            "    compatible with>')\n",
            "  --export_model_name: name of the exported model. Must not contain forward\n",
            "    slashes.\n",
            "    (default: 'model')\n",
            "  --export_model_version: semantic version of the exported model. See\n",
            "    https://semver.org/. This is fully controlled by you as author of the model\n",
            "    and has no required connection with DeepSpeech versions\n",
            "    (default: '0.0.1')\n",
            "  --[no]export_tflite: export a graph ready for TF Lite engine\n",
            "    (default: 'false')\n",
            "  --[no]export_zip: export a TFLite model and package with LM and info.json\n",
            "    (default: 'false')\n",
            "  --feature_cache: cache MFCC features to disk to speed up future training runs\n",
            "    on the same data. This flag specifies the path where cached features\n",
            "    extracted from --train_files will be saved. If empty, or if online\n",
            "    augmentation flags are enabled, caching will be disabled.\n",
            "    (default: '')\n",
            "  --feature_win_len: feature extraction audio window length in milliseconds\n",
            "    (default: '32')\n",
            "    (an integer)\n",
            "  --feature_win_step: feature extraction window step length in milliseconds\n",
            "    (default: '20')\n",
            "    (an integer)\n",
            "  --[no]force_initialize_learning_rate: Force re-initialization of learning rate\n",
            "    which was previously reduced.\n",
            "    (default: 'false')\n",
            "  --inter_op_parallelism_threads: number of inter-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --intra_op_parallelism_threads: number of intra-op parallelism threads - see\n",
            "    tf.ConfigProto for more details. USE OF THIS FLAG IS UNSUPPORTED\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --learning_rate: learning rate of Adam optimizer\n",
            "    (default: '0.001')\n",
            "    (a number)\n",
            "  --limit_dev: maximum number of elements to use from validation set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_test: maximum number of elements to use from test set- 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --limit_train: maximum number of elements to use from train set - 0 means no\n",
            "    limit\n",
            "    (default: '0')\n",
            "    (an integer)\n",
            "  --lm_alpha: the alpha hyperparameter of the CTC decoder. Language Model\n",
            "    weight.\n",
            "    (default: '0.931289039105002')\n",
            "    (a number)\n",
            "  --lm_alpha_max: the maximum of the alpha hyperparameter of the CTC decoder\n",
            "    explored during hyperparameter optimization. Language Model weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --lm_beta: the beta hyperparameter of the CTC decoder. Word insertion weight.\n",
            "    (default: '1.1834137581510284')\n",
            "    (a number)\n",
            "  --lm_beta_max: the maximum beta hyperparameter of the CTC decoder explored\n",
            "    during hyperparameter optimization. Word insertion weight.\n",
            "    (default: '5.0')\n",
            "    (a number)\n",
            "  --load_checkpoint_dir: directory in which checkpoints are stored - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --[no]load_cudnn: Specifying this flag allows one to convert a CuDNN RNN\n",
            "    checkpoint to a checkpoint capable of running on a CPU graph.\n",
            "    (default: 'false')\n",
            "  --load_evaluate: what checkpoint to load for evaluation tasks (test epochs,\n",
            "    model export, single file inference, etc). \"last\" for loading most recent\n",
            "    epoch checkpoint, \"best\" for loading best validation loss checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --load_train: what checkpoint to load before starting the training process.\n",
            "    \"last\" for loading most recent epoch checkpoint, \"best\" for loading best\n",
            "    validation loss checkpoint, \"init\" for initializing a new checkpoint, \"auto\"\n",
            "    for trying several options.\n",
            "    (default: 'auto')\n",
            "  --log_level: log level for console logs - 0: DEBUG, 1: INFO, 2: WARN, 3: ERROR\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]log_placement: whether to log device placement of the operators to the\n",
            "    console\n",
            "    (default: 'false')\n",
            "  --max_to_keep: number of checkpoint files to keep - default value is 5\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --n_hidden: layer width to use when initialising layers\n",
            "    (default: '2048')\n",
            "    (an integer)\n",
            "  --n_steps: how many timesteps to process at once by the export graph, higher\n",
            "    values mean more latency\n",
            "    (default: '16')\n",
            "    (an integer)\n",
            "  --n_trials: the number of trials to run during hyperparameter optimization.\n",
            "    (default: '2400')\n",
            "    (an integer)\n",
            "  --one_shot_infer: one-shot inference mode: specify a wav file and the script\n",
            "    will load the checkpoint and perform inference on it.\n",
            "    (default: '')\n",
            "  --plateau_epochs: Number of epochs to consider for RLROP. Has to be smaller\n",
            "    than es_epochs from early stopping\n",
            "    (default: '10')\n",
            "    (an integer)\n",
            "  --plateau_reduction: Multiplicative factor to apply to the current learning\n",
            "    rate if a plateau has occurred.\n",
            "    (default: '0.1')\n",
            "    (a number)\n",
            "  --random_seed: default random seed that is used to initialize variables\n",
            "    (default: '4568')\n",
            "    (an integer)\n",
            "  --read_buffer: buffer-size for reading samples from datasets (supports file-\n",
            "    size suffixes KB, MB, GB, TB)\n",
            "    (default: '1MB')\n",
            "  --[no]reduce_lr_on_plateau: Enable reducing the learning rate if a plateau is\n",
            "    reached. This is the case if the validation loss did not improve for some\n",
            "    epochs.\n",
            "    (default: 'false')\n",
            "  --relu_clip: ReLU clipping value for non-recurrent layers\n",
            "    (default: '20.0')\n",
            "    (a number)\n",
            "  --[no]remove_export: whether to remove old exported models\n",
            "    (default: 'false')\n",
            "  --report_count: number of phrases for each of best WER, median WER and worst\n",
            "    WER to print out during a WER report\n",
            "    (default: '5')\n",
            "    (an integer)\n",
            "  --save_checkpoint_dir: directory to which checkpoints are saved - defaults to\n",
            "    directory \"deepspeech/checkpoints\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --scorer: Alias for --scorer_path.\n",
            "    (default: 'data/lm/kenlm.scorer')\n",
            "  --scorer_path: path to the external scorer file created with\n",
            "    data/lm/generate_package.py\n",
            "    (default: 'data/lm/kenlm.scorer')\n",
            "  --[no]show_progressbar: Show progress for training, validation and testing\n",
            "    processes. Log level should be > 0.\n",
            "    (default: 'true')\n",
            "  --summary_dir: target directory for TensorBoard summaries - defaults to\n",
            "    directory \"deepspeech/summaries\" within user's data home specified by the\n",
            "    XDG Base Directory Specification\n",
            "    (default: '')\n",
            "  --test_batch_size: number of elements in a test batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --test_files: comma separated list of files specifying the dataset used for\n",
            "    testing. Multiple files will get merged. If empty, the model will not be\n",
            "    tested.\n",
            "    (default: '')\n",
            "  --test_output_file: path to a file to save all src/decoded/distance/loss\n",
            "    tuples generated during a test epoch\n",
            "    (default: '')\n",
            "  --train_batch_size: number of elements in a training batch\n",
            "    (default: '1')\n",
            "    (an integer)\n",
            "  --[no]train_cudnn: use CuDNN RNN backend for training on GPU. Note that\n",
            "    checkpoints created with this flag can only be used with CuDNN RNN, i.e.\n",
            "    fine tuning on a CPU device will not work\n",
            "    (default: 'false')\n",
            "  --train_files: comma separated list of files specifying the dataset used for\n",
            "    training. Multiple files will get merged. If empty, training will not be\n",
            "    run.\n",
            "    (default: '')\n",
            "  --[no]use_allow_growth: use Allow Growth flag which will allocate only\n",
            "    required amount of GPU memory and prevent full allocation of available GPU\n",
            "    memory\n",
            "    (default: 'false')\n",
            "  --[no]utf8: enable UTF-8 mode. When this is used the model outputs UTF-8\n",
            "    sequences directly rather than using an alphabet mapping.\n",
            "    (default: 'false')\n",
            "\n",
            "tensorflow.python.ops.parallel_for.pfor:\n",
            "  --[no]op_conversion_fallback_to_while_loop: If true, falls back to using a\n",
            "    while loop for ops for which a converter is not defined.\n",
            "    (default: 'false')\n",
            "\n",
            "absl.flags:\n",
            "  --flagfile: Insert flag definitions from the given file into the command line.\n",
            "    (default: '')\n",
            "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
            "    the command line even if the program does not define a flag with that name.\n",
            "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
            "    format.\n",
            "    (default: '')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t7-5q1r3nlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEGPuumb3uJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "fc4ea001-0a42-4d47-e7f7-27adf40623c2"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/MyDrive/DS_Test/DeepSpeech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0afRnhh730PS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "73fcf5c3-1ecc-472e-bb45-720d387a65da"
      },
      "source": [
        "#Seems like our audio files are not in the required 'wav' format using soundfile library to change the format\n",
        "!pip install soundfile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.6/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile) (1.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubGeZI1Y4OaV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#changing the files to the required .wav format\n",
        "import librosa\n",
        "import soundfile as sf"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrDN2KV24Q2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,_ = librosa.load('/content/sample_data/wav_files/SX127_new.WAV', sr=16000)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNur5WvS4aiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sf.write('/content/sample_data/wav_files/SX127_new.wav', x, 16000)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9ElYaqP70qJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "95353e9e-2010-4148-d238-df40c408a7bd"
      },
      "source": [
        "#this command helps us to get the size of the 'wav' file wich needs to be provided in csv file\n",
        "os.path.getsize('/content/sample_data/wav_files/SX397_new.wav')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46330"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2xYn6oL4dIW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "813d5b90-0ccf-4a0f-f4c9-04a18129bb8c"
      },
      "source": [
        "wave.open('tmp.wav','r')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wave.Wave_read at 0x7f1e81c922e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMYitY9lzm55",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ffb41274-bc3c-43c6-f9b5-8796cd941a39"
      },
      "source": [
        "#To download DeepSpeech and it's dependencies in one go (already done, can be skippped)\n",
        "\"\"\"\n",
        "!curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
        "!apt-get install git-lfs\n",
        "!git clone --branch v0.7.3 https://github.com/mozilla/DeepSpeech\n",
        "%cd DeepSpeech\n",
        "!pwd\n",
        "!pip3 install --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --upgrade --force-reinstall -e .\n",
        "#!python3 util/taskcluster.py --target .\n",
        "\"\"\""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Detected operating system as Ubuntu/bionic.\n",
            "Checking for curl...\n",
            "Detected curl...\n",
            "Checking for gpg...\n",
            "Detected gpg...\n",
            "Running apt-get update... done.\n",
            "Installing apt-transport-https... done.\n",
            "Installing /etc/apt/sources.list.d/github_git-lfs.list...done.\n",
            "Importing packagecloud gpg key... done.\n",
            "Running apt-get update... done.\n",
            "\n",
            "The repository is setup! You can now install packages.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 43 not upgraded.\n",
            "Need to get 6,877 kB of archives.\n",
            "After this operation, 16.4 MB of additional disk space will be used.\n",
            "Get:1 https://packagecloud.io/github/git-lfs/ubuntu bionic/main amd64 git-lfs amd64 2.11.0 [6,877 kB]\n",
            "Fetched 6,877 kB in 0s (14.9 MB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144383 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.11.0_amd64.deb ...\n",
            "Unpacking git-lfs (2.11.0) ...\n",
            "Setting up git-lfs (2.11.0) ...\n",
            "Git LFS initialized.\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Cloning into 'DeepSpeech'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 19410 (delta 8), reused 19 (delta 6), pack-reused 19370\u001b[K\n",
            "Receiving objects: 100% (19410/19410), 47.95 MiB | 11.20 MiB/s, done.\n",
            "Resolving deltas: 100% (13232/13232), done.\n",
            "Note: checking out '88584941bc2ff5b91d6b11ad0a6b85da391d626b'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "Checking out files: 100% (1901/1901), done.\n",
            "fatal: cannot exec '.git/hooks/post-checkout': Permission denied\n",
            "/content/MyDrive/DS_Test/DeepSpeech\n",
            "/content/MyDrive/DS_Test/DeepSpeech\n",
            "Collecting pip==20.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 3.5MB/s \n",
            "\u001b[?25hRequirement already up-to-date: wheel==0.34.2 in /usr/local/lib/python3.6/dist-packages (0.34.2)\n",
            "Collecting setuptools==46.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
            "\u001b[K     |████████████████████████████████| 583kB 20.1MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip, setuptools\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "  Found existing installation: setuptools 47.3.1\n",
            "    Uninstalling setuptools-47.3.1:\n",
            "      Successfully uninstalled setuptools-47.3.1\n",
            "Successfully installed pip-20.0.2 setuptools-46.1.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/MyDrive/DS_Test/DeepSpeech\n",
            "Collecting tensorflow==1.15.2\n",
            "  Downloading tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 29 kB/s \n",
            "\u001b[?25hCollecting numpy\n",
            "  Downloading numpy-1.19.0-cp36-cp36m-manylinux2010_x86_64.whl (14.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.6 MB 245 kB/s \n",
            "\u001b[?25hCollecting progressbar2\n",
            "  Downloading progressbar2-3.51.4-py2.py3-none-any.whl (25 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pyxdg\n",
            "  Downloading pyxdg-0.26-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting attrdict\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Collecting absl-py\n",
            "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
            "\u001b[K     |████████████████████████████████| 104 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting semver\n",
            "  Downloading semver-2.10.2-py2.py3-none-any.whl (12 kB)\n",
            "Collecting opuslib==2.0.0\n",
            "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
            "Collecting optuna\n",
            "  Downloading optuna-1.5.0.tar.gz (200 kB)\n",
            "\u001b[K     |████████████████████████████████| 200 kB 58.8 MB/s \n",
            "\u001b[?25hCollecting sox\n",
            "  Downloading sox-1.3.7-py2.py3-none-any.whl (34 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.0.5-cp36-cp36m-manylinux1_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 55.7 MB/s \n",
            "\u001b[?25hCollecting requests\n",
            "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 346 kB/s \n",
            "\u001b[?25hCollecting numba==0.47.0\n",
            "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7 MB 38.2 MB/s \n",
            "\u001b[?25hCollecting llvmlite==0.31.0\n",
            "  Downloading llvmlite-0.31.0-cp36-cp36m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.4 MB/s \n",
            "\u001b[?25hCollecting librosa\n",
            "  Downloading librosa-0.7.2.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 50.2 MB/s \n",
            "\u001b[?25hCollecting soundfile\n",
            "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting ds_ctcdecoder@ https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.3.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.3-cp36-cp36m-manylinux1_x86_64.whl\n",
            "  Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.3.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.3-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 968 kB/s \n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 986 kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 60.5 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 41.1 MB/s \n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 58.2 MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26; python_version >= \"3\"\n",
            "  Downloading wheel-0.34.2-py2.py3-none-any.whl (26 kB)\n",
            "Collecting protobuf>=3.6.1\n",
            "  Downloading protobuf-3.12.2-cp36-cp36m-manylinux1_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 52.3 MB/s \n",
            "\u001b[?25hCollecting python-utils>=2.3.0\n",
            "  Downloading python_utils-2.4.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.0 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cliff\n",
            "  Downloading cliff-3.3.0-py3-none-any.whl (81 kB)\n",
            "\u001b[K     |████████████████████████████████| 81 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting cmaes>=0.5.0\n",
            "  Downloading cmaes-0.5.1-py3-none-any.whl (12 kB)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-0.16.0-py3-none-any.whl (300 kB)\n",
            "\u001b[K     |████████████████████████████████| 300 kB 58.4 MB/s \n",
            "\u001b[?25hCollecting scipy!=1.4.0\n",
            "  Downloading scipy-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 40 kB/s \n",
            "\u001b[?25hCollecting sqlalchemy>=1.1.0\n",
            "  Downloading SQLAlchemy-1.3.18-cp36-cp36m-manylinux2010_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 43.7 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.47.0-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.9.1-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 58.3 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.2\n",
            "  Downloading pytz-2020.1-py2.py3-none-any.whl (510 kB)\n",
            "\u001b[K     |████████████████████████████████| 510 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting python-dateutil>=2.6.1\n",
            "  Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 50.7 MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 52.0 MB/s \n",
            "\u001b[?25hCollecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
            "\u001b[K     |████████████████████████████████| 126 kB 58.0 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Using cached setuptools-49.1.0-py3-none-any.whl (789 kB)\n",
            "Collecting audioread>=2.0.0\n",
            "  Downloading audioread-2.1.8.tar.gz (21 kB)\n",
            "Collecting scikit-learn!=0.19.0,>=0.14.0\n",
            "  Downloading scikit_learn-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 44.1 MB/s \n",
            "\u001b[?25hCollecting decorator>=3.0.0\n",
            "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
            "Collecting resampy>=0.2.2\n",
            "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
            "\u001b[K     |████████████████████████████████| 323 kB 65.9 MB/s \n",
            "\u001b[?25hCollecting cffi>=1.0\n",
            "  Downloading cffi-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (399 kB)\n",
            "\u001b[K     |████████████████████████████████| 399 kB 64.1 MB/s \n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 49.1 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting Mako\n",
            "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.0 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting pyparsing>=2.1.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting PrettyTable<0.8,>=0.7.2\n",
            "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
            "Collecting PyYAML>=3.12\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[K     |████████████████████████████████| 269 kB 56.8 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting cmd2!=0.8.3,>=0.8.0\n",
            "  Downloading cmd2-1.1.0-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting stevedore>=1.20.0\n",
            "  Downloading stevedore-2.0.1-py3-none-any.whl (39 kB)\n",
            "Collecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting pycparser\n",
            "  Downloading pycparser-2.20-py2.py3-none-any.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 54.1 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata; python_version < \"3.8\"\n",
            "  Downloading importlib_metadata-1.7.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting MarkupSafe>=0.9.2\n",
            "  Downloading MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (27 kB)\n",
            "Collecting attrs>=16.3.0\n",
            "  Downloading attrs-19.3.0-py2.py3-none-any.whl (39 kB)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
            "Collecting wcwidth>=0.1.7\n",
            "  Downloading wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.1.0-py3-none-any.whl (4.9 kB)\n",
            "Building wheels for collected packages: absl-py, opuslib, optuna, bs4, librosa, gast, wrapt, termcolor, alembic, audioread, resampy, PrettyTable, PyYAML, pyperclip\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=2390aa4cef34dab3547297744a32d510fb21ea9eb2edc497ac9fb9abfbc5d6aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/af/84/3962a6af7b4ab336e951b7877dcfb758cf94548bb1771e0679\n",
            "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=f84d9e6cf745e393160eaf58200bcf544d94d6cdb10e302e8f918254dbb3f177\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/01/88/37797e9e9d157a33eefed22a46aa0bf5044effcec6a9181e41\n",
            "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for optuna: filename=optuna-1.5.0-py3-none-any.whl size=276145 sha256=a1f37f4cab53294fbe27c5d6fe99075e96e47845a01cf65e59557bce46a5ec40\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/27/3d/f8541909296d8618d30d0da35463d98a84d89b60a167b255f4\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=aefa4b3e8ac0a5dd1b956aca847f447155161b9dd318bd5f7f11a44e6154f732\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/6d/a97dd4f22376d4472d5f4c76c7646876052ff3166b3cf71050\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612883 sha256=34f8ab5ce55b3da9fab39bb5ea8d9aa7099fbe179e9b164a9042173d7a3b0998\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/1d/15/a479fa740849128d481333d2f354f97691be3e2c82480a3e00\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=afcf030ea5154e4b5f3ae4d50f5d81199ec232ddc3962038d752f85ec088cbb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67484 sha256=9c5899d36d79a0c3b96608a8995382e767cb23218d63c901e15024dcd8164975\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/42/7f/23cae9ff6ef66798d00dc5d659088e57dbba01566f6c60db63\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=d8d4d9d41bbf821921f588f62a76f564952f889e2ec21f6fd9c3f95fc2c4f85d\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
            "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=db31653cdf33609d1e9d05e567792f69c80ecae0af220bc6dc6cc8d779c8a61f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
            "  Building wheel for audioread (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for audioread: filename=audioread-2.1.8-py3-none-any.whl size=23091 sha256=ba33736828d2f3520801d2e88dabe4dbda576ecce08bdb3396c563334109d8d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/9a/c7/249a5daf5cb90d9786afaec371cba9dc43f04f916db5d1caff\n",
            "  Building wheel for resampy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320720 sha256=bdaa3dfa2f2ac650aaaa8bc9c76bd48229c369212511d20a98463435681c1466\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/d4/04/49d8824a42bd9f9b11d502727965b9997f0d41d2b22ae4f645\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13698 sha256=a81f6b193375138205b8700e405b39a93e540af49d91ffd062310d00529fbf4f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/15/c3/5f28b42ae9c81638570b8b7ed654e0f98c5fdc08875869511b\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=c394ad39fdb497c254a55a20faeab022c3fda475424538d16f88e9a2d15594c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=ab9856378385d6accc0fa94d1561783d4de3de12e628a82d31b5457026f92b06\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/30/fe/92e2d4b1301ba74c07ea09c9e4c08f5bf12bae9c30319d74c5\n",
            "Successfully built absl-py opuslib optuna bs4 librosa gast wrapt termcolor alembic audioread resampy PrettyTable PyYAML pyperclip\n",
            "\u001b[31mERROR: umap-learn 0.4.4 has requirement numba!=0.47,>=0.46, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.12.0, but you'll have six 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, six, keras-preprocessing, gast, h5py, keras-applications, absl-py, zipp, importlib-metadata, markdown, setuptools, protobuf, werkzeug, grpcio, wheel, tensorboard, wrapt, termcolor, opt-einsum, google-pasta, astor, tensorflow-estimator, tensorflow, python-utils, progressbar2, pyxdg, attrdict, semver, opuslib, sqlalchemy, MarkupSafe, Mako, python-editor, python-dateutil, alembic, pyparsing, PrettyTable, PyYAML, pbr, attrs, colorama, pyperclip, wcwidth, cmd2, stevedore, cliff, cmaes, colorlog, joblib, scipy, tqdm, optuna, sox, soupsieve, beautifulsoup4, bs4, pytz, pandas, certifi, chardet, idna, urllib3, requests, llvmlite, numba, audioread, threadpoolctl, scikit-learn, decorator, resampy, pycparser, cffi, soundfile, librosa, ds-ctcdecoder, deepspeech-training\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.12.0\n",
            "    Uninstalling six-1.12.0:\n",
            "      Successfully uninstalled six-1.12.0\n",
            "  Attempting uninstall: keras-preprocessing\n",
            "    Found existing installation: Keras-Preprocessing 1.1.2\n",
            "    Uninstalling Keras-Preprocessing-1.1.2:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.1.2\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Attempting uninstall: keras-applications\n",
            "    Found existing installation: Keras-Applications 1.0.8\n",
            "    Uninstalling Keras-Applications-1.0.8:\n",
            "      Successfully uninstalled Keras-Applications-1.0.8\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 0.9.0\n",
            "    Uninstalling absl-py-0.9.0:\n",
            "      Successfully uninstalled absl-py-0.9.0\n",
            "  Attempting uninstall: zipp\n",
            "    Found existing installation: zipp 3.1.0\n",
            "    Uninstalling zipp-3.1.0:\n",
            "      Successfully uninstalled zipp-3.1.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 1.6.1\n",
            "    Uninstalling importlib-metadata-1.6.1:\n",
            "      Successfully uninstalled importlib-metadata-1.6.1\n",
            "  Attempting uninstall: markdown\n",
            "    Found existing installation: Markdown 3.2.2\n",
            "    Uninstalling Markdown-3.2.2:\n",
            "      Successfully uninstalled Markdown-3.2.2\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 46.1.3\n",
            "    Uninstalling setuptools-46.1.3:\n",
            "      Successfully uninstalled setuptools-46.1.3\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.10.0\n",
            "    Uninstalling protobuf-3.10.0:\n",
            "      Successfully uninstalled protobuf-3.10.0\n",
            "  Attempting uninstall: werkzeug\n",
            "    Found existing installation: Werkzeug 1.0.1\n",
            "    Uninstalling Werkzeug-1.0.1:\n",
            "      Successfully uninstalled Werkzeug-1.0.1\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.30.0\n",
            "    Uninstalling grpcio-1.30.0:\n",
            "      Successfully uninstalled grpcio-1.30.0\n",
            "  Attempting uninstall: wheel\n",
            "    Found existing installation: wheel 0.34.2\n",
            "    Uninstalling wheel-0.34.2:\n",
            "      Successfully uninstalled wheel-0.34.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 1.1.0\n",
            "    Uninstalling termcolor-1.1.0:\n",
            "      Successfully uninstalled termcolor-1.1.0\n",
            "  Attempting uninstall: opt-einsum\n",
            "    Found existing installation: opt-einsum 3.2.1\n",
            "    Uninstalling opt-einsum-3.2.1:\n",
            "      Successfully uninstalled opt-einsum-3.2.1\n",
            "  Attempting uninstall: google-pasta\n",
            "    Found existing installation: google-pasta 0.2.0\n",
            "    Uninstalling google-pasta-0.2.0:\n",
            "      Successfully uninstalled google-pasta-0.2.0\n",
            "  Attempting uninstall: astor\n",
            "    Found existing installation: astor 0.8.1\n",
            "    Uninstalling astor-0.8.1:\n",
            "      Successfully uninstalled astor-0.8.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.2.0\n",
            "    Uninstalling tensorflow-2.2.0:\n",
            "      Successfully uninstalled tensorflow-2.2.0\n",
            "  Attempting uninstall: python-utils\n",
            "    Found existing installation: python-utils 2.4.0\n",
            "    Uninstalling python-utils-2.4.0:\n",
            "      Successfully uninstalled python-utils-2.4.0\n",
            "  Attempting uninstall: progressbar2\n",
            "    Found existing installation: progressbar2 3.38.0\n",
            "    Uninstalling progressbar2-3.38.0:\n",
            "      Successfully uninstalled progressbar2-3.38.0\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 1.3.17\n",
            "    Uninstalling SQLAlchemy-1.3.17:\n",
            "      Successfully uninstalled SQLAlchemy-1.3.17\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 1.1.1\n",
            "    Uninstalling MarkupSafe-1.1.1:\n",
            "      Successfully uninstalled MarkupSafe-1.1.1\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.8.1\n",
            "    Uninstalling python-dateutil-2.8.1:\n",
            "      Successfully uninstalled python-dateutil-2.8.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 2.4.7\n",
            "    Uninstalling pyparsing-2.4.7:\n",
            "      Successfully uninstalled pyparsing-2.4.7\n",
            "  Attempting uninstall: PrettyTable\n",
            "    Found existing installation: prettytable 0.7.2\n",
            "    Uninstalling prettytable-0.7.2:\n",
            "      Successfully uninstalled prettytable-0.7.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 19.3.0\n",
            "    Uninstalling attrs-19.3.0:\n",
            "      Successfully uninstalled attrs-19.3.0\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.5\n",
            "    Uninstalling wcwidth-0.2.5:\n",
            "      Successfully uninstalled wcwidth-0.2.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 0.15.1\n",
            "    Uninstalling joblib-0.15.1:\n",
            "      Successfully uninstalled joblib-0.15.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "  Attempting uninstall: bs4\n",
            "    Found existing installation: bs4 0.0.1\n",
            "    Uninstalling bs4-0.0.1:\n",
            "      Successfully uninstalled bs4-0.0.1\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2018.9\n",
            "    Uninstalling pytz-2018.9:\n",
            "      Successfully uninstalled pytz-2018.9\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.0.5\n",
            "    Uninstalling pandas-1.0.5:\n",
            "      Successfully uninstalled pandas-1.0.5\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2020.6.20\n",
            "    Uninstalling certifi-2020.6.20:\n",
            "      Successfully uninstalled certifi-2020.6.20\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 2.9\n",
            "    Uninstalling idna-2.9:\n",
            "      Successfully uninstalled idna-2.9\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Attempting uninstall: audioread\n",
            "    Found existing installation: audioread 2.1.8\n",
            "    Uninstalling audioread-2.1.8:\n",
            "      Successfully uninstalled audioread-2.1.8\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "  Attempting uninstall: resampy\n",
            "    Found existing installation: resampy 0.2.2\n",
            "    Uninstalling resampy-0.2.2:\n",
            "      Successfully uninstalled resampy-0.2.2\n",
            "  Attempting uninstall: pycparser\n",
            "    Found existing installation: pycparser 2.20\n",
            "    Uninstalling pycparser-2.20:\n",
            "      Successfully uninstalled pycparser-2.20\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.14.0\n",
            "    Uninstalling cffi-1.14.0:\n",
            "      Successfully uninstalled cffi-1.14.0\n",
            "  Attempting uninstall: librosa\n",
            "    Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "  Running setup.py develop for deepspeech-training\n",
            "Successfully installed Mako-1.1.3 MarkupSafe-1.1.1 PrettyTable-0.7.2 PyYAML-5.3.1 absl-py-0.9.0 alembic-1.4.2 astor-0.8.1 attrdict-2.0.1 attrs-19.3.0 audioread-2.1.8 beautifulsoup4-4.9.1 bs4-0.0.1 certifi-2020.6.20 cffi-1.14.0 chardet-3.0.4 cliff-3.3.0 cmaes-0.5.1 cmd2-1.1.0 colorama-0.4.3 colorlog-4.1.0 decorator-4.4.2 deepspeech-training ds-ctcdecoder-0.7.3 gast-0.2.2 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 idna-2.10 importlib-metadata-1.7.0 joblib-0.16.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 librosa-0.7.2 llvmlite-0.31.0 markdown-3.2.2 numba-0.47.0 numpy-1.19.0 opt-einsum-3.2.1 optuna-1.5.0 opuslib-2.0.0 pandas-1.0.5 pbr-5.4.5 progressbar2-3.51.4 protobuf-3.12.2 pycparser-2.20 pyparsing-2.4.7 pyperclip-1.8.0 python-dateutil-2.8.1 python-editor-1.0.4 python-utils-2.4.0 pytz-2020.1 pyxdg-0.26 requests-2.24.0 resampy-0.2.2 scikit-learn-0.23.1 scipy-1.5.0 semver-2.10.2 setuptools-49.1.0 six-1.15.0 soundfile-0.10.3.post1 soupsieve-2.0.1 sox-1.3.7 sqlalchemy-1.3.18 stevedore-2.0.1 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1 termcolor-1.1.0 threadpoolctl-2.1.0 tqdm-4.47.0 urllib3-1.25.9 wcwidth-0.2.5 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cffi",
                  "chardet",
                  "dateutil",
                  "decorator",
                  "google",
                  "grpc",
                  "idna",
                  "numpy",
                  "pandas",
                  "pkg_resources",
                  "pyparsing",
                  "pytz",
                  "requests",
                  "six",
                  "tqdm",
                  "urllib3",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqJlOVGi0k7u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "53975097-9eea-4d66-83ff-24a55c7d6978"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/sample_data/DeepSpeech'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7O7EI9JXO1W",
        "colab_type": "text"
      },
      "source": [
        "Training the model.\n",
        "We will create three csv files as highlighted in the [link](https#https://github.com/mozilla/DeepSpeech/wiki). We have use the scorer file downloaded above for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZbRsFp8BxEr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "95a8c9be-16de-4fa8-d20c-34128d102adb"
      },
      "source": [
        "!python3 DeepSpeech.py --train_files /content/sample_data/wav_files/csv_file_train.csv --dev_files /content/sample_data/wav_files/csv_file_validate.csv --test_files /content/sample_data/wav_files/csv_file_test.csv --scorer_path /content/sample_data/deepspeech-0.7.4-models.scorer --export_dir /content/sample_data --epochs 30"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "swig/python detected a memory leak of type 'Alphabet *', no destructor found.\n",
            "I Loading best validating checkpoint from /root/.local/share/deepspeech/checkpoints/best_dev-35\n",
            "I Loading variable from checkpoint: beta1_power\n",
            "I Loading variable from checkpoint: beta2_power\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias/Adam_1\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel/Adam_1\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam\n",
            "I Loading variable from checkpoint: layer_1/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam\n",
            "I Loading variable from checkpoint: layer_1/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam\n",
            "I Loading variable from checkpoint: layer_2/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam\n",
            "I Loading variable from checkpoint: layer_2/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam\n",
            "I Loading variable from checkpoint: layer_3/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam\n",
            "I Loading variable from checkpoint: layer_3/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam\n",
            "I Loading variable from checkpoint: layer_5/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam\n",
            "I Loading variable from checkpoint: layer_5/weights/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam\n",
            "I Loading variable from checkpoint: layer_6/bias/Adam_1\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam\n",
            "I Loading variable from checkpoint: layer_6/weights/Adam_1\n",
            "I Loading variable from checkpoint: learning_rate\n",
            "I STARTING Optimization\n",
            "Epoch 0 |   Training | Elapsed Time: 0:02:13 | Steps: 5 | Loss: 224.437213      \n",
            "Epoch 0 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 143.815206 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "I Saved new best validating model with loss 143.815206 to: /root/.local/share/deepspeech/checkpoints/best_dev-40\n",
            "Epoch 1 |   Training | Elapsed Time: 0:02:13 | Steps: 5 | Loss: 222.835864      \n",
            "Epoch 1 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.162865 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 2 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 222.814917      \n",
            "Epoch 2 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.548246 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 3 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 223.426834      \n",
            "Epoch 3 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.541537 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 4 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 222.845728      \n",
            "Epoch 4 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.651123 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 5 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 222.668665      \n",
            "Epoch 5 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.494815 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 6 |   Training | Elapsed Time: 0:02:13 | Steps: 5 | Loss: 222.887555      \n",
            "Epoch 6 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.420832 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 7 |   Training | Elapsed Time: 0:02:13 | Steps: 5 | Loss: 223.302191      \n",
            "Epoch 7 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.573072 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 8 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 222.752402      \n",
            "Epoch 8 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.300438 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 9 |   Training | Elapsed Time: 0:02:13 | Steps: 5 | Loss: 223.269189      \n",
            "Epoch 9 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.248647 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 10 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 223.357590     \n",
            "Epoch 10 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.334061 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 11 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 223.154700     \n",
            "Epoch 11 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.153534 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 12 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 222.548016     \n",
            "Epoch 12 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.523196 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 13 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 223.882263     \n",
            "Epoch 13 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.541046 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 14 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 222.756265     \n",
            "Epoch 14 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.403384 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 15 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 223.239694     \n",
            "Epoch 15 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.345896 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 16 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 223.299600     \n",
            "Epoch 16 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.561437 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 17 |   Training | Elapsed Time: 0:02:14 | Steps: 5 | Loss: 223.190225     \n",
            "Epoch 17 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.411174 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 18 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 223.639349     \n",
            "Epoch 18 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.321950 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 19 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 223.279068     \n",
            "Epoch 19 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.369565 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 20 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 222.788535     \n",
            "Epoch 20 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.408450 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 21 |   Training | Elapsed Time: 0:02:19 | Steps: 5 | Loss: 223.011346     \n",
            "Epoch 21 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.468379 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 22 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 223.283154     \n",
            "Epoch 22 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.609433 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 23 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 223.291568     \n",
            "Epoch 23 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.589640 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 24 |   Training | Elapsed Time: 0:02:15 | Steps: 5 | Loss: 223.094476     \n",
            "Epoch 24 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.358187 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 25 |   Training | Elapsed Time: 0:02:17 | Steps: 5 | Loss: 223.511426     \n",
            "Epoch 25 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.381147 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 26 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 222.690158     \n",
            "Epoch 26 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.578440 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 27 |   Training | Elapsed Time: 0:02:17 | Steps: 5 | Loss: 223.245593     \n",
            "Epoch 27 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.720690 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 28 |   Training | Elapsed Time: 0:02:16 | Steps: 5 | Loss: 222.970126     \n",
            "Epoch 28 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.529912 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "Epoch 29 |   Training | Elapsed Time: 0:02:17 | Steps: 5 | Loss: 222.856335     \n",
            "Epoch 29 | Validation | Elapsed Time: 0:00:03 | Steps: 3 | Loss: 144.632083 | Dataset: /content/sample_data/wav_files/csv_file_validate.csv\n",
            "I FINISHED optimization in 1:10:34.665268\n",
            "swig/python detected a memory leak of type 'Alphabet *', no destructor found.\n",
            "I Loading best validating checkpoint from /root/.local/share/deepspeech/checkpoints/best_dev-40\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: global_step\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "Testing model on /content/sample_data/wav_files/csv_file_test.csv\n",
            "Test epoch | Steps: 0 | Elapsed Time: 0:00:00                                   swig/python detected a memory leak of type 'Output *', no destructor found.\n",
            "swig/python detected a memory leak of type 'std::vector< Output,std::allocator< Output > > *', no destructor found.\n",
            "swig/python detected a memory leak of type 'std::vector< std::vector< Output,std::allocator< Output > > > *', no destructor found.\n",
            "swig/python detected a memory leak of type 'Alphabet *', no destructor found.\n",
            "Test epoch | Steps: 1 | Elapsed Time: 0:00:04                                   swig/python detected a memory leak of type 'Output *', no destructor found.\n",
            "swig/python detected a memory leak of type 'std::vector< Output,std::allocator< Output > > *', no destructor found.\n",
            "swig/python detected a memory leak of type 'std::vector< std::vector< Output,std::allocator< Output > > > *', no destructor found.\n",
            "swig/python detected a memory leak of type 'Alphabet *', no destructor found.\n",
            "Test epoch | Steps: 2 | Elapsed Time: 0:00:05                                   \n",
            "Test on /content/sample_data/wav_files/csv_file_test.csv - WER: 0.928571, CER: 0.971831, loss: 147.473846\n",
            "--------------------------------------------------------------------------------\n",
            "Best WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.888889, CER: 0.976744, loss: 164.644623\n",
            " - wav: file:///content/sample_data/wav_files/SX397_new.wav\n",
            " - src: \"tim takes sheila to see movies twice a week\"\n",
            " - res: \"a\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.964286, loss: 130.303070\n",
            " - wav: file:///content/sample_data/wav_files/SX307_new.wav\n",
            " - src: \"the meeting is now adjourned\"\n",
            " - res: \"a\"\n",
            "--------------------------------------------------------------------------------\n",
            "Median WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.964286, loss: 130.303070\n",
            " - wav: file:///content/sample_data/wav_files/SX307_new.wav\n",
            " - src: \"the meeting is now adjourned\"\n",
            " - res: \"a\"\n",
            "--------------------------------------------------------------------------------\n",
            "Worst WER: \n",
            "--------------------------------------------------------------------------------\n",
            "WER: 0.888889, CER: 0.976744, loss: 164.644623\n",
            " - wav: file:///content/sample_data/wav_files/SX397_new.wav\n",
            " - src: \"tim takes sheila to see movies twice a week\"\n",
            " - res: \"a\"\n",
            "--------------------------------------------------------------------------------\n",
            "WER: 1.000000, CER: 0.964286, loss: 130.303070\n",
            " - wav: file:///content/sample_data/wav_files/SX307_new.wav\n",
            " - src: \"the meeting is now adjourned\"\n",
            " - res: \"a\"\n",
            "--------------------------------------------------------------------------------\n",
            "I Exporting the model...\n",
            "I Loading best validating checkpoint from /root/.local/share/deepspeech/checkpoints/best_dev-40\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
            "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
            "I Loading variable from checkpoint: layer_1/bias\n",
            "I Loading variable from checkpoint: layer_1/weights\n",
            "I Loading variable from checkpoint: layer_2/bias\n",
            "I Loading variable from checkpoint: layer_2/weights\n",
            "I Loading variable from checkpoint: layer_3/bias\n",
            "I Loading variable from checkpoint: layer_3/weights\n",
            "I Loading variable from checkpoint: layer_5/bias\n",
            "I Loading variable from checkpoint: layer_5/weights\n",
            "I Loading variable from checkpoint: layer_6/bias\n",
            "I Loading variable from checkpoint: layer_6/weights\n",
            "I Models exported at /content/sample_data\n",
            "I Model metadata file saved to /content/sample_data/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiKWbzLGD3EY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4fe2865c-36be-4c9d-bacf-5d1903029202"
      },
      "source": [
        "pwd"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'/content/sample_data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N3nrqtkXWht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "46b02b5a-9a13-4e80-a25e-d4ca0b2cde8e"
      },
      "source": [
        "cd .."
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kdqaMeuXbAI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "a20e9b7e-7714-4011-8e44-0440c3b849d4"
      },
      "source": [
        "#Our model is trained and stored in the required path mentiod above\n",
        "#Transcribe an audio file with the new model trained\n",
        "!deepspeech --model output_graph.pb --scorer /content/sample_data/deepspeech-0.7.4-models.scorer --audio /content/sample_data/wav_files/SA1_new.wav"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model from file output_graph.pb\n",
            "TensorFlow: v1.15.0-24-gceb46aa\n",
            "DeepSpeech: v0.7.4-0-gfcd9563\n",
            "Warning: reading entire model file into memory. Transform model file into an mmapped graph to reduce heap usage.\n",
            "2020-07-04 14:00:29.638116: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "Loaded model in 0.148s.\n",
            "Loading scorer from files /content/sample_data/deepspeech-0.7.4-models.scorer\n",
            "Loaded scorer in 0.000255s.\n",
            "Running inference.\n",
            "2020-07-04 14:00:29.813800: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 134217728 exceeds 10% of system memory.\n",
            "2020-07-04 14:00:29.972313: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 134217728 exceeds 10% of system memory.\n",
            "2020-07-04 14:00:30.235133: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 134217728 exceeds 10% of system memory.\n",
            "2020-07-04 14:00:30.304340: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 134217728 exceeds 10% of system memory.\n",
            "2020-07-04 14:00:30.375353: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 134217728 exceeds 10% of system memory.\n",
            "a\n",
            "Inference took 3.387s for 2.925s audio file.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}